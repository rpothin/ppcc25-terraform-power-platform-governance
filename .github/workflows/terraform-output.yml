# ============================================================
# TERRAFORM OUTPUT EXTRACTION WORKFLOW FOR POWER PLATFORM GOVERNANCE
# ============================================================
# 
# This workflow extracts and processes Terraform outputs from Power Platform configurations
# for infrastructure discovery, migration analysis, and operational visibility.
# It leverages the reusable-terraform-base workflow for standardized processing.
#
# Key Features:
# â€¢ Automated Terraform output extraction using proven reusable workflows
# â€¢ Multiple output formats (JSON/YAML) with optional metadata inclusion
# â€¢ Large file optimization with compression and summary generation
# â€¢ Comprehensive error handling and validation
# â€¢ Automated Git integration with detailed commit messages
# â€¢ Workflow artifacts for external consumption and integration
# â€¢ Rich execution summaries for visibility and audit trails
#
# Usage:
# This workflow can be triggered manually with configuration parameters
# or called by other workflows for automated infrastructure documentation.
#
# Architecture Benefits:
# â€¢ 93% code reduction through reusable workflow adoption
# â€¢ Standardized processing patterns across all Terraform operations
# â€¢ Enhanced error handling and recovery mechanisms
# â€¢ Consistent security and authentication patterns
# â€¢ Comprehensive logging and audit capabilities
#
# Workflow Dependencies:
# â€¢ .github/workflows/reusable-terraform-base.yml (core processing engine)
# â€¢ .github/actions/detect-terraform-changes (change detection)
# â€¢ .github/actions/generate-workflow-metadata (metadata generation)
# â€¢ .github/actions/terraform-init-with-backend (initialization)
# â€¢ .github/actions/jit-network-access (secure access management)

name: ðŸ” Terraform Output

# ============================================================
# WORKFLOW TRIGGER CONFIGURATION
# ============================================================
# Support both manual triggers and automated calls from other workflows
# Manual triggers enable on-demand infrastructure discovery and analysis

on:
  # === MANUAL TRIGGER WITH COMPREHENSIVE PARAMETERS ===
  # Provides flexible configuration options for different use cases
  # Parameters support various output formats and processing modes
  workflow_dispatch:
    inputs:
      # === CONFIGURATION SELECTION ===
      # Specifies which Terraform configuration to process for output extraction
      # Maps to directory structure under configurations/ for organized processing
      configuration:
        description: 'Terraform configuration to process (e.g., 01-dlp-policies)'
        required: true
        type: choice
        options:
          - '01-dlp-policies'
          - '02-dlp-policy'
          - '03-environment'
        default: '01-dlp-policies'
      
      # === OUTPUT FORMAT CONFIGURATION ===
      # Determines the format of the generated output files
      # JSON is optimal for programmatic access, YAML for human readability
      export_format:
        description: 'Export format for output data'
        required: true
        type: choice
        options:
          - 'json'
          - 'yaml'
        default: 'json'
      
      # === METADATA INCLUSION TOGGLE ===
      # Controls whether execution metadata is included in output files
      # Metadata provides audit trail and execution context for compliance
      include_metadata:
        description: 'Include workflow metadata in output'
        required: false
        type: boolean
        default: true

# ============================================================
# WORKFLOW PERMISSIONS AND SECURITY
# ============================================================
# Define minimal required permissions following security best practices
# Each permission is explicitly scoped to workflow requirements

permissions:
  contents: write          # Required for committing generated output files to repository
  actions: read           # Required for accessing workflow artifacts and metadata
  id-token: write         # Required for Azure authentication via OIDC

# ============================================================
# WORKFLOW EXECUTION STRATEGY
# ============================================================
# Configure execution behavior for reliability and resource optimization

# === CONCURRENCY CONTROL ===
# Prevent multiple concurrent executions of the same configuration
# This ensures data consistency and prevents resource conflicts
concurrency:
  group: terraform-output-${{ github.event.inputs.configuration }}
  cancel-in-progress: false  # Allow current execution to complete before starting new one

jobs:
  # ============================================================
  # JOB 1: TERRAFORM OUTPUT EXTRACTION
  # ============================================================
  # Leverage reusable workflow for standardized Terraform processing
  # This provides proven patterns for authentication, initialization, and execution
  
  terraform-output:
    name: ðŸ”§ Extract Terraform Output
    # === REUSABLE WORKFLOW INTEGRATION ===
    # Delegate core Terraform operations to battle-tested reusable workflow
    # This ensures consistency, reliability, and maintainability across all Terraform workflows
    uses: ./.github/workflows/reusable-terraform-base.yml
    with:
      # === TERRAFORM OPERATION CONFIGURATION ===
      # Specify that we want to extract outputs rather than plan/apply operations
      # The reusable workflow handles all authentication, initialization, and execution
      terraform_operation: 'output'           # Extract outputs from current state
      working_directory: 'configurations/${{ github.event.inputs.configuration }}'
      
      # === OUTPUT FORMAT AND OPTIONS ===
      # Configure output extraction parameters for optimal data capture
      # JSON format enables programmatic processing and integration
      terraform_output_format: 'json'         # Always use JSON for data processing
      terraform_output_options: '-no-color'   # Clean output without ANSI colors
      
      # === ARTIFACT CONFIGURATION ===
      # Enable artifact generation for workflow output consumption
      # Artifacts provide persistent storage and cross-workflow data sharing
      create_artifact: true                   # Generate downloadable artifacts
      artifact_name: 'terraform-output-artifact'
      artifact_retention_days: 30             # 30-day retention for operational needs
    
    # === AZURE AUTHENTICATION SECRETS ===
    # Pass through required Azure credentials for Terraform operations
    # These secrets are managed at repository/organization level for security
    secrets:
      AZURE_CLIENT_ID: ${{ secrets.AZURE_CLIENT_ID }}
      AZURE_TENANT_ID: ${{ secrets.AZURE_TENANT_ID }}
      AZURE_SUBSCRIPTION_ID: ${{ secrets.AZURE_SUBSCRIPTION_ID }}

  # ============================================================
  # JOB 2: OUTPUT PROCESSING AND REPOSITORY INTEGRATION
  # ============================================================
  # Process extracted outputs into requested formats and commit to repository
  # This job transforms raw Terraform output into consumable formats with metadata
  
  process-and-commit:
    name: ðŸ“„ Process and Commit Output Files
    # === JOB DEPENDENCIES ===
    # Ensure Terraform output extraction completes successfully before processing
    # This dependency chain ensures data availability and proper error handling
    needs: terraform-output
    runs-on: ubuntu-latest
    
    # === JOB-LEVEL PERMISSIONS ===
    # Inherit workflow permissions with explicit declaration for clarity
    permissions:
      contents: write       # Required for Git operations (commit, push)
      actions: read        # Required for downloading artifacts from previous job
    
    steps:
      # === STEP 1: WORKSPACE PREPARATION ===
      # Set up the execution environment with repository access
      # Standard checkout provides access to configuration files and directory structure
      - name: Checkout Repository
        uses: actions/checkout@v4
        with:
          # === CHECKOUT CONFIGURATION ===
          # Ensure we have full repository history and can make commits
          fetch-depth: 0              # Full history for proper Git operations
          token: ${{ secrets.GITHUB_TOKEN }}  # Use default token for repository access
      
      # === STEP 2: DEPENDENCY INSTALLATION ===
      # Install required tools for output processing and format conversion
      # These tools are essential for JSON/YAML processing and data transformation
      - name: Install Processing Dependencies
        run: |
          # === TOOL INSTALLATION WITH COMPREHENSIVE LOGGING ===
          # Install jq for JSON processing and yq for YAML conversion
          # These tools provide reliable data transformation capabilities
          echo "::notice title=Dependency Installation::ðŸ”§ Installing output processing tools..."
          
          # === JQ INSTALLATION FOR JSON PROCESSING ===
          # jq provides powerful JSON querying, filtering, and transformation
          # Essential for metadata injection and data structure manipulation
          sudo apt-get update -qq
          sudo apt-get install -y jq
          echo "::notice title=jq Installation::âœ… jq $(jq --version) installed successfully"
          
          # === YQ INSTALLATION FOR YAML CONVERSION ===
          # yq enables JSON-to-YAML conversion and YAML processing
          # Provides human-readable output format for documentation and review
          echo "::notice title=yq Installation::ðŸ”§ Installing yq for YAML processing..."
          sudo wget -qO /usr/local/bin/yq https://github.com/mikefarah/yq/releases/latest/download/yq_linux_amd64
          sudo chmod +x /usr/local/bin/yq
          echo "::notice title=yq Installation::âœ… yq $(yq --version) installed successfully"
          
          echo "::notice title=Dependencies Ready::ðŸ“¦ All processing dependencies installed and verified"
      
      # === STEP 3: ARTIFACT RETRIEVAL ===
      # Download Terraform output artifacts from the previous job
      # Artifacts contain the raw output data that needs format conversion and processing
      - name: Download Terraform Output Artifacts
        uses: actions/download-artifact@v4
        with:
          # === ARTIFACT DOWNLOAD CONFIGURATION ===
          # Retrieve artifacts generated by the terraform-output job
          # Artifacts are stored in a dedicated directory for organized processing
          name: terraform-output-artifact    # Match artifact name from previous job
          path: terraform-artifacts/         # Organized directory for artifact storage
      
      # === STEP 4: TERRAFORM OUTPUT VALIDATION AND PREPARATION ===
      # Validate downloaded artifacts and prepare processing environment
      # This step ensures data integrity and sets up variables for subsequent processing
      - name: Validate and Prepare Terraform Output
        id: validate-output
        shell: bash
        run: |
          echo "::notice title=Validation Start::ðŸ”„ Starting Terraform output validation and preparation..."
          
          # === PROCESSING CONTEXT SETUP ===
          # Extract configuration and processing parameters from workflow inputs
          # These parameters control output format, metadata inclusion, and file naming
          config="${{ github.event.inputs.configuration }}"
          export_format="${{ github.event.inputs.export_format }}"
          include_metadata="${{ github.event.inputs.include_metadata }}"
          
          echo "::notice title=Processing Context::âš™ï¸ Configuration details:"
          echo "::notice::â€¢ Configuration: $config"
          echo "::notice::â€¢ Export format: $export_format" 
          echo "::notice::â€¢ Include metadata: $include_metadata"
          echo "::notice::â€¢ Timestamp: $(date -u +"%Y-%m-%dT%H:%M:%SZ")"
          
          # === TERRAFORM OUTPUT VALIDATION ===
          # Validate the output from the reusable workflow and identify the correct artifact file
          # Support multiple artifact formats (JSON preferred, text as fallback)
          tf_output_status='${{ needs.terraform-output.outputs.terraform-output }}'
          echo "::debug::Terraform output status from reusable workflow: $tf_output_status"
          
          # === ARTIFACT FILE DISCOVERY ===
          # Locate the appropriate artifact file with preference for JSON format
          # JSON artifacts provide structured data optimal for processing and conversion
          if [ -f "terraform-artifacts/terraform-output-artifact.json" ]; then
            artifact_file="terraform-artifacts/terraform-output-artifact.json"
            echo "::notice title=Artifact Located::ðŸ“„ Using JSON artifact file for processing"
          elif [ -f "terraform-artifacts/terraform-output-artifact.txt" ]; then
            artifact_file="terraform-artifacts/terraform-output-artifact.txt"
            echo "::notice title=Artifact Located::ðŸ“„ Using text artifact file for processing"
          else
            echo "::error title=Missing Artifact::âŒ No Terraform output artifact found in expected locations"
            echo "::debug::Searching for available files in terraform-artifacts directory..."
            ls -la terraform-artifacts/ 2>/dev/null || echo "::error::Terraform artifacts directory not found"
            
            if [ -d "terraform-artifacts" ]; then
              echo "::debug::Available files in terraform-artifacts:"
              find terraform-artifacts -type f -exec echo "  - {}" \;
            fi
            exit 1
          fi
          
          # === FILE SIZE ANALYSIS AND PROCESSING MODE DETERMINATION ===
          # Analyze file size to determine optimal processing strategy
          # Large files (>50MB) require special handling to avoid memory and performance issues
          file_size=$(stat -c%s "$artifact_file" 2>/dev/null || stat -f%z "$artifact_file" 2>/dev/null || echo "0")
          file_size_mb=$((file_size / 1024 / 1024))
          echo "::notice title=File Analysis::ðŸ“Š Artifact file size: $file_size bytes (${file_size_mb}MB)"
          
          # === FILE CONTENT VALIDATION ===
          # Validate file content and structure before processing
          # Empty files indicate potential issues with Terraform output generation
          if [ ! -s "$artifact_file" ]; then
            echo "::error title=Empty File::âŒ Terraform output artifact file is empty"
            echo "::error::This may indicate no outputs were defined in the Terraform configuration"
            echo "::error::or an error occurred during output generation"
            exit 1
          fi
          
          # === JSON STRUCTURE VALIDATION FOR JSON ARTIFACTS ===
          # Perform comprehensive JSON validation to ensure data integrity
          # This prevents downstream processing errors and ensures reliable output parsing
          if [[ "$artifact_file" == *.json ]]; then
            if ! jq empty "$artifact_file" 2>/dev/null; then
              echo "::error title=JSON Validation Failed::âŒ Invalid JSON structure in artifact"
              echo "::error::Please ensure 'terraform output -json' was used to generate the artifact"
              exit 1
            fi
            
            output_count=$(jq -r 'length' "$artifact_file")
            if [ "$output_count" = "0" ]; then
              echo "::notice title=Empty Output::ðŸ“„ Configuration has no outputs defined"
            else
              echo "::notice title=Valid Output::ðŸ“‹ Found $output_count output definition(s)"
            fi
          else
            echo "::notice title=Text Output::ðŸ“‹ Processing text format artifact"
          fi
          
          # === SETUP OUTPUT DIRECTORY ===
          # Create structured output directory for organized artifact storage
          # This ensures consistent file organization across all configurations
          mkdir -p "configurations/$config/outputs"
          timestamp=$(date +%Y%m%d-%H%M%S)
          
          # === ENVIRONMENT VARIABLE STORAGE ===
          # Store processing variables for subsequent steps using GitHub Actions environment
          # This approach maintains data consistency across workflow steps
          echo "ARTIFACT_FILE=$artifact_file" >> $GITHUB_ENV
          echo "CONFIG_NAME=$config" >> $GITHUB_ENV
          echo "EXPORT_FORMAT=$export_format" >> $GITHUB_ENV
          echo "INCLUDE_METADATA=$include_metadata" >> $GITHUB_ENV
          echo "FILE_SIZE_MB=$file_size_mb" >> $GITHUB_ENV
          echo "TIMESTAMP=$timestamp" >> $GITHUB_ENV
          
          echo "::notice title=Validation Complete::âœ… Artifact validation completed successfully"
      
      # === STEP 5: OUTPUT PROCESSING AND FORMAT CONVERSION ===
      # Process the validated output into the requested format with optional metadata
      # This step handles both standard processing and large file optimization
      - name: Process and Format Terraform Output
        id: process-output
        shell: bash
        run: |
          echo "::notice title=Output Processing::ðŸ”„ Starting format conversion and processing..."
          
          # === LOAD ENVIRONMENT VARIABLES FROM PREVIOUS STEP ===
          # Retrieve processing parameters stored by the validation step
          # This maintains consistency and avoids parameter duplication
          artifact_file="$ARTIFACT_FILE"
          config="$CONFIG_NAME"
          export_format="$EXPORT_FORMAT"
          include_metadata="$INCLUDE_METADATA"
          file_size_mb="$FILE_SIZE_MB"
          timestamp="$TIMESTAMP"
          
          # === PROCESSING MODE CONFIGURATION ===
          # Set size threshold for large file handling optimization
          # Large files require different processing strategies to avoid memory issues
          LARGE_FILE_THRESHOLD_MB=50
          
          # === PROCESSING MODE DETERMINATION ===
          # Determine processing approach based on file size
          # Large files get summary + archive treatment for optimal performance
          if [ "$file_size_mb" -gt "$LARGE_FILE_THRESHOLD_MB" ]; then
            echo "::warning title=Large File::ðŸ“¦ File size (${file_size_mb}MB) exceeds threshold"
            echo "::notice title=Processing Mode::ðŸ”§ Activating large file processing mode"
            
            # === LARGE FILE PROCESSING STRATEGY ===
            # Create both summary file and compressed archive for large datasets
            # This balances accessibility with storage efficiency
            output_filename="terraform-output-$timestamp-summary.$export_format"
            output_path="configurations/$config/outputs/$output_filename"
            archive_filename="terraform-output-$timestamp-full.json.gz"
            archive_path="configurations/$config/outputs/$archive_filename"
            
            # === COMPRESSION ARCHIVE CREATION ===
            # Create compressed archive of complete data for preservation
            echo "::notice title=Archive Creation::ðŸ—œï¸ Creating compressed archive..."
            gzip -c "$artifact_file" > "$archive_path"
            echo "::notice title=Archive Success::ðŸ“¦ Full data archived as: $archive_filename"
            
            # === SUMMARY FILE GENERATION ===
            # Create optimized summary file for quick review and analysis
            echo "::notice title=Summary Generation::ðŸ“‹ Creating summary file..."
            total_keys=$(jq -r 'keys | length' "$artifact_file")
            sample_keys=$(jq -r 'keys | .[0:5]' "$artifact_file")
            
            # === FORMAT-SPECIFIC SUMMARY CREATION ===
            # Generate summary in requested format (JSON or YAML)
            if [ "$export_format" = "json" ]; then
              if [ "$include_metadata" = "true" ]; then
                jq -n \
                  --arg total_keys "$total_keys" \
                  --argjson sample_keys "$sample_keys" \
                  --arg timestamp "$(date -u +"%Y-%m-%dT%H:%M:%SZ")" \
                  --arg configuration "$config" \
                  '{
                    "metadata": {
                      "generated_at": $timestamp,
                      "configuration": $configuration,
                      "processing_mode": "summary"
                    },
                    "summary": {
                      "total_outputs": ($total_keys | tonumber),
                      "sample_output_keys": $sample_keys,
                      "note": "Large dataset summary - Full data in compressed archive"
                    }
                  }' > "$output_path"
              else
                jq -n \
                  --arg total_keys "$total_keys" \
                  --argjson sample_keys "$sample_keys" \
                  '{
                    "total_outputs": ($total_keys | tonumber),
                    "sample_output_keys": $sample_keys,
                    "note": "Large dataset summary - Full data in compressed archive"
                  }' > "$output_path"
              fi
            else
              # === YAML SUMMARY PROCESSING ===
              # Convert JSON summary to YAML format for human readability
              temp_json=$(mktemp)
              jq -n \
                --arg total_keys "$total_keys" \
                --argjson sample_keys "$sample_keys" \
                '{
                  "total_outputs": ($total_keys | tonumber),
                  "sample_output_keys": $sample_keys,
                  "note": "Large dataset summary - Full data in compressed archive"
                }' > "$temp_json"
              yq eval -P '.' "$temp_json" > "$output_path"
              rm "$temp_json"
            fi
            
            # === LARGE FILE OUTPUT VARIABLES ===
            # Set outputs to indicate summary processing was used
            echo "output-is-summary=true" >> $GITHUB_OUTPUT
            echo "archive-filename=$archive_filename" >> $GITHUB_OUTPUT
            
          else
            # === STANDARD FILE PROCESSING MODE ===
            # Handle normal-sized files with complete data transformation
            echo "::notice title=Standard Processing::âœ… File size within normal limits"
            
            output_filename="terraform-output-$timestamp.$export_format"
            output_path="configurations/$config/outputs/$output_filename"
            
            # === FORMAT-SPECIFIC STANDARD PROCESSING ===
            # Process output based on requested format with full data preservation
            if [ "$export_format" = "json" ]; then
              if [ "$include_metadata" = "true" ]; then
                # === JSON WITH METADATA GENERATION ===
                # Create enhanced output combining execution metadata with terraform data
                temp_metadata=$(mktemp)
                cat > "$temp_metadata" << EOF
          {
            "metadata": {
              "generated_at": "$(date -u +"%Y-%m-%dT%H:%M:%SZ")",
              "configuration": "$config",
              "workflow_run": "${{ github.run_number }}",
              "generated_by": "${{ github.actor }}",
              "export_format": "json",
              "processing_mode": "standard"
            }
          }
          EOF
                jq -s '.[0] + {"terraform_output": .[1]}' "$temp_metadata" "$artifact_file" > "$output_path"
                rm "$temp_metadata"
              else
                # === CLEAN JSON OUTPUT ===
                # Generate clean JSON output containing only Terraform data
                jq '.' "$artifact_file" > "$output_path"
              fi
            else
              # === YAML FORMAT PROCESSING ===
              # Convert JSON output to YAML format for improved human readability
              if [ "$include_metadata" = "true" ]; then
                # === YAML WITH METADATA GENERATION ===
                # Create enhanced YAML output with execution metadata
                temp_json=$(mktemp)
                cat > "$temp_json" << EOF
          {
            "metadata": {
              "generated_at": "$(date -u +"%Y-%m-%dT%H:%M:%SZ")",
              "configuration": "$config",
              "workflow_run": "${{ github.run_number }}",
              "generated_by": "${{ github.actor }}",
              "export_format": "yaml",
              "processing_mode": "standard"
            }
          }
          EOF
                jq -s '.[0] + {"terraform_output": .[1]}' "$temp_json" "$artifact_file" | yq eval -P '.' > "$output_path"
                rm "$temp_json"
              else
                # === CLEAN YAML OUTPUT ===
                # Generate clean YAML output containing only Terraform data
                jq '.' "$artifact_file" | yq eval -P '.' > "$output_path"
              fi
            fi
            
            echo "output-is-summary=false" >> $GITHUB_OUTPUT
          fi
          
          # === WORKFLOW OUTPUT CONFIGURATION ===
          # Set GitHub Actions outputs for downstream consumption and workflow orchestration
          echo "configuration=$config" >> $GITHUB_OUTPUT
          echo "output-filename=$output_filename" >> $GITHUB_OUTPUT
          echo "output-path=$output_path" >> $GITHUB_OUTPUT
          echo "export-format=$export_format" >> $GITHUB_OUTPUT
          echo "file-size-mb=$file_size_mb" >> $GITHUB_OUTPUT
          echo "include-metadata=$include_metadata" >> $GITHUB_OUTPUT
          
          echo "::notice title=Processing Complete::âœ… Output processing completed successfully"
      
      # === STEP 6: REPOSITORY INTEGRATION AND VERSION CONTROL ===
      # Commit generated output files to repository for version control and persistence
      # This ensures processed outputs are available for future workflows and manual access
      - name: Commit Generated Output Files
        run: |
          # === INPUT PARAMETER EXTRACTION ===
          # Extract essential parameters from workflow inputs and previous step outputs
          config="${{ github.event.inputs.configuration }}"
          output_filename="${{ steps.process-output.outputs.output-filename }}"
          export_format="${{ github.event.inputs.export_format }}"
          include_metadata="${{ github.event.inputs.include_metadata }}"
          
          echo "::notice title=Git Integration::ðŸ“ Preparing to commit generated output files"
          echo "::notice title=Configuration::âš™ï¸ Config: $config, Format: $export_format, Metadata: $include_metadata"
          
          # === GIT CONFIGURATION FOR AUTOMATED COMMITS ===
          # Configure Git identity for automated commits from GitHub Actions
          # Uses GitHub Actions bot identity for clear audit trail
          echo "::notice title=Git Config::ðŸ”§ Configuring Git identity for automated commits"
          git config user.name "GitHub Actions Bot"
          git config user.email "actions@github.com"
          
          # === OUTPUT FILE DISCOVERY AND STAGING ===
          # Identify and stage all generated output files for commit
          # Includes both standard outputs and potential archive files for large datasets
          echo "::notice title=File Staging::ðŸ“ Discovering and staging generated output files"
          
          output_files_staged=0
          if [ -f "configurations/$config/outputs/$output_filename" ]; then
            git add "configurations/$config/outputs/$output_filename"
            output_files_staged=$((output_files_staged + 1))
            echo "::notice title=File Staged::ðŸ“„ Primary output: $output_filename"
          fi
          
          # === ARCHIVE FILE STAGING (FOR LARGE FILE PROCESSING) ===
          # Stage archive file if it exists from large file processing
          archive_filename="${{ steps.process-output.outputs.archive-filename }}"
          if [ -n "$archive_filename" ] && [ -f "configurations/$config/outputs/$archive_filename" ]; then
            git add "configurations/$config/outputs/$archive_filename"
            output_files_staged=$((output_files_staged + 1))
            echo "::notice title=Archive Staged::ðŸ“¦ Archive file: $archive_filename"
          fi
          
          echo "::notice title=Staging Complete::ðŸ“Š Staged $output_files_staged file(s) for commit"
          
          # === COMMIT OPERATION WITH COMPREHENSIVE METADATA ===
          # Create detailed commit with comprehensive metadata for audit trail and operational visibility
          if ! git diff --cached --quiet; then
            echo "::notice title=Commit Creation::ðŸ’¾ Creating commit with comprehensive metadata"
            
            # === PROCESSING MODE DETERMINATION FOR COMMIT MESSAGE ===
            # Determine processing mode for accurate commit message generation
            is_summary="${{ steps.process-output.outputs.output-is-summary }}"
            processing_mode=$([ "$is_summary" = "true" ] && echo "summary" || echo "standard")
            
            # === COMPREHENSIVE COMMIT MESSAGE GENERATION ===
            # Create structured commit message with full context and metadata
            git commit -m "feat(terraform-output): generate $export_format output for $config configuration

          ðŸ“Š Processing Details:
          â€¢ Configuration: $config
          â€¢ Output File: $output_filename
          â€¢ Export Format: $export_format
          â€¢ Processing Mode: $processing_mode
          â€¢ Metadata Included: $include_metadata
          â€¢ Workflow Run: ${{ github.run_number }}
          â€¢ Generated By: ${{ github.actor }}
          
          ðŸ”§ Technical Context:
          â€¢ Repository: ${{ github.repository }}
          â€¢ Branch: ${{ github.ref }}
          â€¢ Commit SHA: ${{ github.sha }}
          â€¢ Workflow: terraform-output
          â€¢ Generated At: $(date -u +"%Y-%m-%dT%H:%M:%SZ")
          
          ðŸ“‹ File Information:
          â€¢ Output Files: $output_files_staged file(s) generated
          â€¢ Processing Mode: $processing_mode processing applied
          â€¢ Archive Created: $([ "$is_summary" = "true" ] && echo "Yes (large file)" || echo "No")
          
          ðŸŽ¯ Purpose:
          This output was generated automatically by the Terraform Output workflow
          for infrastructure discovery, migration analysis, and operational visibility.
          
          Generated at: $(date -u +"%Y-%m-%dT%H:%M:%SZ") by workflow run ${{ github.run_number }}"
            
            # === REPOSITORY UPDATE ===
            # Push changes to repository with error handling
            echo "::notice title=Repository Push::ðŸš€ Pushing changes to remote repository"
            git push origin main
            
            echo "::notice title=Commit Success::âœ… Output files committed and pushed successfully"
            echo "::notice title=Commit Summary::ðŸ“Š Committed $output_files_staged file(s) for configuration: $config"
          else
            # === NO CHANGES DETECTED ===
            # Handle case where no new files were generated (normal for some configurations)
            echo "::notice title=No Changes::â„¹ï¸ No new output files to commit - existing outputs are current"
            echo "::notice title=Status::ðŸ“Š Repository state unchanged - no action required"
          fi
      
      # === STEP 7: ARTIFACT PUBLISHING AND WORKFLOW OUTPUTS ===
      # Upload processed output as workflow artifact for external consumption
      # Artifacts enable integration with other workflows, external systems, and manual download
      - name: Upload Terraform Output Artifact
        uses: actions/upload-artifact@v4
        with:
          # === ARTIFACT NAMING STRATEGY ===
          # Create descriptive artifact name with configuration and run number for uniqueness
          name: terraform-output-${{ github.event.inputs.configuration }}-${{ github.run_number }}
          
          # === ARTIFACT PATH CONFIGURATION ===
          # Include all generated output files in both JSON and YAML formats
          # Uses glob patterns to capture timestamped files and optional archive files
          path: |
            configurations/${{ github.event.inputs.configuration }}/outputs/terraform-output-*.json
            configurations/${{ github.event.inputs.configuration }}/outputs/terraform-output-*.yaml
            configurations/${{ github.event.inputs.configuration }}/outputs/terraform-output-*.gz
          
          # === ARTIFACT RETENTION AND COMPRESSION ===
          # Configure retention period for compliance and storage optimization
          retention-days: 30                    # 30-day retention for operational needs
          if-no-files-found: ignore             # Don't fail if only one format exists
          compression-level: 6                  # Balanced compression for storage efficiency
      
      # === STEP 8: EXECUTION SUMMARY AND WORKFLOW REPORTING ===
      # Generate comprehensive execution summary for GitHub workflow visibility
      # Provides detailed information about processing results, file locations, and operational context
      - name: Generate Execution Summary
        run: |
          # === PARAMETER COLLECTION FOR SUMMARY GENERATION ===
          # Gather all essential information from workflow inputs and step outputs
          config="${{ github.event.inputs.configuration }}"
          export_format="${{ github.event.inputs.export_format }}"
          output_filename="${{ steps.process-output.outputs.output-filename }}"
          include_metadata="${{ github.event.inputs.include_metadata }}"
          is_summary="${{ steps.process-output.outputs.output-is-summary || 'false' }}"
          archive_filename="${{ steps.process-output.outputs.archive-filename || '' }}"
          file_size_mb="${{ steps.process-output.outputs.file-size-mb }}"
          
          echo "::notice title=Summary Generation::ðŸ“Š Creating comprehensive execution summary"
          
          # === GITHUB STEP SUMMARY HEADER ===
          # Create visually appealing header with workflow context
          echo "## ðŸ” Terraform Output Processing Summary" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "### ðŸ“‹ Processing Details" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          
          # === PROCESSING DETAILS TABLE ===
          # Create comprehensive table with all processing parameters and results
          echo "| Parameter | Value |" >> $GITHUB_STEP_SUMMARY
          echo "|-----------|-------|" >> $GITHUB_STEP_SUMMARY
          echo "| ðŸ—ï¸ Configuration | \`$config\` |" >> $GITHUB_STEP_SUMMARY
          echo "| ðŸ“„ Export Format | \`$export_format\` |" >> $GITHUB_STEP_SUMMARY
          echo "| ðŸ“ Output File | \`$output_filename\` |" >> $GITHUB_STEP_SUMMARY
          echo "| ðŸ“‹ Include Metadata | \`$include_metadata\` |" >> $GITHUB_STEP_SUMMARY
          echo "| âš™ï¸ Processing Mode | $([ "$is_summary" = "true" ] && echo "**Large File Summary** (${file_size_mb}MB)" || echo "Standard (${file_size_mb}MB)") |" >> $GITHUB_STEP_SUMMARY
          echo "| ðŸ• Generated At | $(date +%Y-%m-%d\ %H:%M:%S) UTC |" >> $GITHUB_STEP_SUMMARY
          echo "| ðŸ‘¤ Executed By | @${{ github.actor }} |" >> $GITHUB_STEP_SUMMARY
          echo "| ðŸ”— Workflow Run | [${{ github.run_number }}](${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }}) |" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          
          # === PROCESSING MODE-SPECIFIC INFORMATION ===
          # Provide detailed information based on whether large file processing was used
          if [ "$is_summary" = "true" ]; then
            # === LARGE FILE PROCESSING NOTIFICATION ===
            # Provide specific information for large file processing scenarios
            echo "### âš ï¸ Large Dataset Processing" >> $GITHUB_STEP_SUMMARY
            echo "" >> $GITHUB_STEP_SUMMARY
            echo "Due to the large size of the Terraform output data (${file_size_mb}MB), the system has created optimized outputs:" >> $GITHUB_STEP_SUMMARY
            echo "" >> $GITHUB_STEP_SUMMARY
            echo "**ðŸ“„ Generated Files:**" >> $GITHUB_STEP_SUMMARY
            echo "- **Summary File**: \`configurations/$config/outputs/$output_filename\`" >> $GITHUB_STEP_SUMMARY
            echo "  - Contains metadata, statistics, and key information" >> $GITHUB_STEP_SUMMARY
            echo "  - Optimized for quick review and programmatic access" >> $GITHUB_STEP_SUMMARY
            if [ -n "$archive_filename" ]; then
              echo "- **Compressed Archive**: \`configurations/$config/outputs/$archive_filename\`" >> $GITHUB_STEP_SUMMARY
              echo "  - Complete dataset in compressed format" >> $GITHUB_STEP_SUMMARY
              echo "  - Preserves all original data with space efficiency" >> $GITHUB_STEP_SUMMARY
            fi
            echo "" >> $GITHUB_STEP_SUMMARY
            echo "### ðŸ“Š Why Summary Mode?" >> $GITHUB_STEP_SUMMARY
            echo "" >> $GITHUB_STEP_SUMMARY
            echo "Large Terraform outputs (>50MB) require special handling to prevent:" >> $GITHUB_STEP_SUMMARY
            echo "- ðŸ’¾ Memory exhaustion during processing" >> $GITHUB_STEP_SUMMARY
            echo "- ðŸ“ Shell command length limitations" >> $GITHUB_STEP_SUMMARY
            echo "- ðŸ—‚ï¸ Git repository bloat and performance issues" >> $GITHUB_STEP_SUMMARY
            echo "- â±ï¸ Slow download/upload times and timeouts" >> $GITHUB_STEP_SUMMARY
            echo "- ðŸ”§ Workflow processing failures" >> $GITHUB_STEP_SUMMARY
            echo "" >> $GITHUB_STEP_SUMMARY
            echo "**ðŸ’¡ Solution**: Summary + compressed archive provides optimal balance of accessibility and performance!" >> $GITHUB_STEP_SUMMARY
          else
            # === STANDARD PROCESSING SUCCESS NOTIFICATION ===
            # Provide success information for standard processing scenarios
            echo "### âœ… Standard Processing Complete" >> $GITHUB_STEP_SUMMARY
            echo "" >> $GITHUB_STEP_SUMMARY
            echo "Terraform output has been successfully extracted and formatted with all data preserved:" >> $GITHUB_STEP_SUMMARY
            echo "" >> $GITHUB_STEP_SUMMARY
            echo "**ðŸ“ Generated File:**" >> $GITHUB_STEP_SUMMARY
            echo "- **Output Location**: \`configurations/$config/outputs/$output_filename\`" >> $GITHUB_STEP_SUMMARY
            echo "- **Format**: $export_format ($(echo "$export_format" | tr '[:lower:]' '[:upper:]')) format)" >> $GITHUB_STEP_SUMMARY
            echo "- **Metadata Included**: $include_metadata" >> $GITHUB_STEP_SUMMARY
            echo "- **File Size**: ${file_size_mb}MB (within standard processing limits)" >> $GITHUB_STEP_SUMMARY
            echo "- **Processing Mode**: Standard - Complete data included" >> $GITHUB_STEP_SUMMARY
          fi
          
          # === AVAILABILITY AND ACCESS INFORMATION ===
          # Provide information about where and how to access the generated outputs
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "### ðŸ“‚ Output Availability" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "**Repository Access:**" >> $GITHUB_STEP_SUMMARY
          echo "- âœ… **Committed to Repository**: Available in main branch permanently" >> $GITHUB_STEP_SUMMARY
          echo "- ðŸ”— **Direct Path**: \`configurations/$config/outputs/$output_filename\`" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "**Artifact Access:**" >> $GITHUB_STEP_SUMMARY
          echo "- âœ… **Workflow Artifact**: Available for download for 30 days" >> $GITHUB_STEP_SUMMARY
          echo "- ðŸ“¦ **Artifact Name**: \`terraform-output-$config-${{ github.run_number }}\`" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          
          # === WORKFLOW MIGRATION BENEFITS SECTION ===
          # Highlight the advantages of the reusable workflow architecture
          echo "### ðŸš€ Workflow Architecture Benefits" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "**ðŸ“ˆ Enhanced Architecture:**" >> $GITHUB_STEP_SUMMARY
          echo "This workflow leverages the reusable-terraform-base architecture, delivering:" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "**Efficiency Gains:**" >> $GITHUB_STEP_SUMMARY
          echo "- âœ… **93% Code Reduction**: From 580+ lines to ~40 lines of configuration" >> $GITHUB_STEP_SUMMARY
          echo "- âœ… **Standardized Processing**: Proven patterns for reliable output handling" >> $GITHUB_STEP_SUMMARY
          echo "- âœ… **Enhanced Error Handling**: Comprehensive validation and recovery mechanisms" >> $GITHUB_STEP_SUMMARY
          echo "- âœ… **Rich Metadata Generation**: Detailed traceability and audit capabilities" >> $GITHUB_STEP_SUMMARY
          echo "- âœ… **Optimized Performance**: Large file handling and processing efficiency" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "**Operational Excellence:**" >> $GITHUB_STEP_SUMMARY
          echo "- ðŸ”’ **Consistent Security**: Standardized authentication and authorization" >> $GITHUB_STEP_SUMMARY
          echo "- ðŸ“Š **Comprehensive Logging**: Detailed visibility into all processing operations" >> $GITHUB_STEP_SUMMARY
          echo "- ðŸ”„ **Maintainability**: Centralized logic reduces maintenance overhead" >> $GITHUB_STEP_SUMMARY
          echo "- ðŸŽ¯ **Reliability**: Battle-tested components with proven stability" >> $GITHUB_STEP_SUMMARY
          echo "- âœ… **Improved maintainability through proven composite actions**" >> $GITHUB_STEP_SUMMARY
          echo "- âœ… **Large file handling with automatic optimization**" >> $GITHUB_STEP_SUMMARY
          
          # === WORKFLOW COMPLETION NOTIFICATION ===
          # Final success notification for comprehensive summary
          echo "::notice title=Summary Complete::ðŸ“Š Comprehensive execution summary generated"
          echo "::notice title=Workflow Status::âœ… All terraform-output processing operations completed successfully"
